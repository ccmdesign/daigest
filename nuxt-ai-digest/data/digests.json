[
  {
    "id": "e7bdee98-e5ba-45b1-a2ed-11f5429548d7",
    "createdAt": "2025-09-21T16:50:05.099Z",
    "summary": {
      "total": 9,
      "durationMs": 84129
    },
    "records": [
      {
        "url": "https://natlawreview.com/article/artificial-intelligence-provisions-fiscal-year-2026-house-and-senate-national",
        "confidence": 85,
        "reason": "All key fields captured. | Playwright rendering failed: page.goto: Timeout 30000ms exceeded.\nCall log:\n\u001b[2m  - navigating to \"https://natlawreview.com/article/artificial-intelligence-provisions-fiscal-year-2026-house-and-senate-national\", waiting until \"networkidle\"\u001b[22m\n",
        "provider": "readability",
        "provenance": {
          "title": "readability",
          "description": "readability",
          "author": "readability",
          "publisher": "readability",
          "published_on": "readability",
          "publication_date": "readability",
          "body": "readability",
          "word_count": "readability",
          "tags": "readability",
          "language": "readability"
        },
        "providersUsed": [
          "basic-http",
          "readability"
        ],
        "redacted": false,
        "title": "AI in FY 2026 National Defense Authorization Acts",
        "description": "Both the US House of Representatives and the US Senate have continued to increase the attention paid to artificial intelligence (AI) issues for the defense sector, most notably by including a number of provisions in the text of the National Defense Authorization Act (NDAA) for Fiscal Year 2026 (FY 2026), alongside the Senate’s version.",
        "author": "Scott J. Gelbman",
        "published_on": "2025-09-11T19:15:30.000Z",
        "publication_date": "2025-09-11T19:15:30.000Z",
        "body": "<div id=\"readability-page-1\" class=\"page\"><div>\n  \n    \n      <p><span>Artificial Intelligence Provisions in the Fiscal Year 2026 House and Senate National Defense Authorization Acts</span>\n\n    </p>\n  </div><div>\n  \n    \n      <p><span>Thursday, September 11, 2025</span>\n\n    </p>\n  </div><div><p>Both the US House of Representatives and the US Senate have continued to increase the attention paid to artificial intelligence (AI) issues for the defense sector, most notably by including a number of provisions in the text of the National Defense Authorization Act (NDAA) for Fiscal Year 2026 (FY 2026), alongside the Senate’s&nbsp;<a href=\"https://www.congress.gov/bill/119th-congress/senate-bill/2296/text\">version</a>. The Chairman’s Mark of the July 2025&nbsp;<a href=\"https://armedservices.house.gov/uploadedfiles/chairmans_mark.pdf\">text</a>&nbsp;of the NDAA notes that the House Armed Services Committee “is aware of the rapidly changing capabilities of [AI] and recognizes its expanding potential for application across the Department of Defense.” The House bill emphasizes the widespread impact of AI across administrative, international, and research functions. On the Senate side, the bill stresses the long-term capabilities of AI creating opportunities for experimentation, model development, and risk frameworks. The House Armed Services Committee passed their version of the NDAA with a vote of 55-2 and the Senate Armed Services Committee advanced its version of the bill on 9 July 2025, after a 26-1 vote. Both chambers will spend the first two weeks of September debating their respective NDAAs on the floor while considering hundreds of amendments. It is highly likely both bills will succeed in getting off the floor but the House version will be more partisan. The bills will be reconciled during the conference process before final passage in both chambers and ultimately signed into law by the president. Stakeholders should closely monitor this process, as key provisions related to AI could shift during conference negotiations or floor amendments.</p>\n<h4>Key Takeaways:</h4>\n<ul>\n<li>Both the House and Senate versions of the FY 2026 NDAA prioritize the adoption and integration of AI across military operations, logistics, and mission-critical applications.&nbsp;</li>\n<li>Both the House and Senate highlight the importance of workforce development, with AI education, cybersecurity training, and advanced manufacturing skills, while the Senate also establishes experimental sandbox environments for training and model development.</li>\n<li>Although governance is a main priority in both the House and Senate, Senate places an emphasized focus on standardized frameworks, risk-based security measures, and supply-chain oversight.</li>\n</ul>\n<h4>AI Education and Training</h4>\n<p>In Section 822, the House bill establishes a working group to address workforce shortages in advanced manufacturing, including AI, and encourages public-private partnerships to incentivize government and industry participation. The House Armed Services Committee also adds a renewed focus for the Department of Defense’s (DoD) annual cybersecurity training to include the unique challenges related to AI (Section 1512). The Senate bill complements this approach in Section 1622 by establishing a taskforce within the DoD to create an AI sandbox environment for experimentation, training, and model development. This taskforce is intended to accelerate responsible AI adoption and strengthen public-private partnerships.&nbsp;</p>\n<h4>AI Governance, Oversight, and Security&nbsp;</h4>\n<p>The House bill emphasizes modernization and security in technology policy. Section 1074 outlines a framework to modernize the technology transfer policies of the military departments and update the National Disclosure Policy, which governs the sharing of classified military information to foreign governments and international organizations. It lists detailed guidelines for security considerations for information sharing between US allies and partners. It further calls for the adoption of industry-recognized frameworks to guide best practices, established standards for governance, and specific training requirements to mitigate vulnerabilities specific to AI and machine learning (Section 1531). The bill also directs the DoD to establish requirements for managing “biological data” generated through DoD-funded research in a way that supports the development and use of AI technologies (Section 1521). Although Section 1521 does not explicitly define “biological data,” it instructs the Secretary of Defense to develop a definition for “qualified biological data resource” based on several criteria: (1) the type of biological data generated, (2) the size of the data collection, (3) the amount of federal funding awarded to the research, (4) the sensitivity level of the data, and (5) any other factor the Secretary deems appropriate. &nbsp;</p>\n<p>The Senate bill includes several provisions to strengthen AI governance and security. It calls for a standardized model assessment and oversight framework (Section 1623), a Department-wide ontology governance working group to ensure data interoperability (Section 1624), and a steering committee to evaluate the strategic implications of AI intelligence (Section 1626). Further, the Senate bill mandates risk-based cybersecurity and physical security requirements for AI systems (Section 1627), prohibits the use of certain foreign-developed AI technologies (Section 1628), and directs the Secretary of Defense to develop digital content provenance standards to safeguard the integrity of AI-generated media (Section 1629). It also includes &nbsp;provisions to create a public-private cybersecurity partnership focused on advanced AI systems (Section 1621) as well as secure digital sandbox environments for testing and experimentation (Section 1622).</p>\n<h4>Deployment and Operational Methods for AI Research and Development</h4>\n<p>In Section 1532, the House bill calls for accelerated utilization of AI in military operations and coordination by launching pilot programs for the Army, Navy, and Air Force branches. These programs would employ commercial AI solutions to improve ground vehicle maintenance. The DoD is also required to produce up to 12 generative AI tools to support mission-critical areas such as damage assessment, cybersecurity, mission analysis, and others (Section 1533).&nbsp;</p>\n<p>Additionally, Section 328 of the Senate bill directs the Secretary of Defense to integrate commercially available AI tools specifically for logistics tracking, planning, operations, and analytics into at least two exercises during FY26. This section mirrors the House’s focus on incorporating commercial AI into logistics operations to test and evaluate AI tools in operational contexts.&nbsp;</p>\n<h4>Unique House AI Initiatives&nbsp;</h4>\n<p>In Title X and XVIII, the House Armed Services Committee creates broad, yet firm calls for the survey and accelerated adoption of AI technologies. In Title X, the DoD must evaluate and survey all current AI technologies in use to find areas to be improved in terms of accuracy and reducing collateral damage. Additionally, in Title XVIII, the DoD is authorized to accelerate autonomy-enabling software across defense programs using middle-tier acquisition authorities allowed by Section 3603 of Title X. Lastly, the House bill uniquely targets international cooperation. In Section 1202, the bill establishes an emerging technology cooperation program with certain allies to conduct joint research, development, testing, and evaluation in critical areas such as AI, cybersecurity, robotics, quantum, and automation.&nbsp;</p>\n<h4>Unique Senate AI Initiatives</h4>\n<p>The Senate NDAA establishes targeted initiatives for AI across national security domains. Section 3118 limits AI research within the National Nuclear Security Administration to support nuclear security missions, while allowing resource sharing with other agencies. Section 1602 directs the commander of United States Cyber Command, in coordination with DoD AI leadership and research offices, to develop a roadmap for industry collaboration on AI-enabled cyberspace operations. This roadmap will guide private sector engagement and the integration of advanced AI into cyber operations.&nbsp;</p>\n<h4>Conclusion</h4>\n<p>The House and Senate Armed Services Committees are not the only congressional bodies focused on advancing AI federal policy. Multiple committees across jurisdictions have actively engaged in this effort, holding hearings and drafting legislation aimed at shaping the future of AI governance.&nbsp;</p>\n<p>In parallel, the Trump administration recently introduced its AI Action Plan along with a robust AI export strategy (see our alerts on the AI Action Plan and AI Export Strategy for further details). We anticipate continued momentum in both Congress and the Executive Branch in the months ahead.&nbsp;</p>\n<p>Our Policy and Regulatory practice team is closely monitoring both legislative and regulatory developments and is ready to help advocate for your policy priorities in this rapidly evolving landscape.</p>\n</div></div>",
        "word_count": 1244,
        "publisher": "National Law Review",
        "language": "eng",
        "tags": [
          "artificial",
          "intelligence",
          "provisions",
          "fiscal",
          "year",
          "house",
          "senate",
          "national",
          "defense",
          "authorization"
        ],
        "notes": [
          "Playwright rendering failed: page.goto: Timeout 30000ms exceeded.\nCall log:\n\u001b[2m  - navigating to \"https://natlawreview.com/article/artificial-intelligence-provisions-fiscal-year-2026-house-and-senate-national\", waiting until \"networkidle\"\u001b[22m\n"
        ]
      },
      {
        "url": "https://rebootdemocracy.ai/blog/NJ-AI-2025",
        "confidence": 90,
        "reason": "All key fields captured.",
        "provider": "readability",
        "provenance": {
          "title": "readability",
          "description": "readability",
          "author": "readability",
          "publisher": "readability",
          "published_on": "readability",
          "publication_date": "readability",
          "body": "readability",
          "word_count": "readability",
          "tags": "readability",
          "language": "readability"
        },
        "providersUsed": [
          "playwright",
          "readability"
        ],
        "redacted": false,
        "title": "From Interim to Institution: New Jersey’s Three-Pillar Strategy for Responsible AI",
        "description": "With its new 2025 AI policy, New Jersey has advanced from interim guidance that encouraged experimentation to a framework that enables safe, large-scale use. Building on two years of training and adoption by more than 15,000 public servants, the state is now focused on using AI not just to streamline work, but to deliver better services where it matters most.",
        "author": "Read Bio →",
        "published_on": "2025-09-15T19:00:00.000Z",
        "publication_date": "2025-09-15T19:00:00.000Z",
        "body": "<div id=\"readability-page-1\" class=\"page\"><div><article role=\"article\" aria-labelledby=\"article-title\"><h2 dir=\"ltr\" id=\"heading-1\"><span>From Interim to Institution: New Jersey’s Three-Pillar Strategy for Responsible AI</span></h2>\n<p dir=\"ltr\">In July, Code for America <a href=\"https://www.nj.gov/governor/news/news/562025/approved/20250715d.shtml\" target=\"_blank\" rel=\"noopener noreferrer\">named New Jersey<span> (opens in new window)</span></a> one of only three states with “advanced” AI readiness. That honor recognizes how we have combined policy, access, and training into a coherent strategy for using AI responsibly in government.</p>\n<p dir=\"ltr\">We were the first state to issue an AI use policy back in 2023. Importantly, that guidance was always meant to be interim. Its purpose was to encourage responsible experimentation at a moment when generative AI tools were brand new and untested in government.</p>\n<p dir=\"ltr\">We knew staff needed both permission and guardrails to start exploring how these technologies might make their work faster, clearer, or more accessible to the public.</p>\n<p dir=\"ltr\">Two years later, more than 15,000 state employees—over one in five—are using generative AI. They’ve logged hundreds of thousands of prompts in the NJ AI Assistant, and they’ve completed training that equips them to use the technology ethically. With this scale of adoption, it was time to update our policy.&nbsp;</p>\n<p dir=\"ltr\">This month, New Jersey’s Chief Technology Officer, working together with the State’s Office of Innovation,<a href=\"https://nj.gov/it/docs/ps/25-OIT-001-State-of-New-Jersey-Guidance-on-Responsible-Use-of-Generative-AI.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"> issued version 2 of the State’s AI Policy.<span> (opens in new window)</span></a></p>\n<h2 dir=\"ltr\" id=\"heading-2\"><span>What’s New in the 2025 Guidance</span></h2>\n<p dir=\"ltr\">The revised guidance moves from a spirit of exploration to one of institutionalization, shifting from “try it out” to “scale it safely.”</p>\n<ul>\n<li dir=\"ltr\" aria-level=\"1\">\n<div dir=\"ltr\" role=\"presentation\"><p><strong>Training is now required</strong>.</p><p>“Before accessing or using generative AI in their official capacity, all state employees should take the ‘Responsible AI for Public Professionals’ course available in the New Jersey Civil Service Commission Learning Management System.”</p><p>In New Jersey, we designed our own training and shared it freely with other states through the <a href=\"https://innovate-us.org/\" target=\"_blank\" rel=\"noopener noreferrer\">InnovateUS<span> (opens in new window)</span></a> initiative</p></div>\n</li>\n<li dir=\"ltr\" aria-level=\"1\">\n<div dir=\"ltr\" role=\"presentation\"><p><strong>High-risk uses require clearance</strong>.</p><p>“The use of resident-facing or decisional generative AI systems must be cleared by the State Chief Technology Officer or their delegate and registered with the NJ Office of Information Technology… When resident-facing or decisional generative AI systems are used, disclosure of generative AI use must be displayed prominently to the user.”</p><p>In other words, public professionals should—and are—using AI for their own productivity, but heightened scrutiny must be applied for uses that directly impact residents.</p></div>\n</li>\n<li dir=\"ltr\" aria-level=\"1\">\n<div dir=\"ltr\" role=\"presentation\"><p><strong>Secure environments for sensitive data.</strong></p><p>“Sensitive Personally Identifiable Information… may only be used under the following conditions: the tool used is a State-Approved AI Tool such as the NJ AI Assistant… [and] the tool use is approved by your Agency Chief Information Officer.”</p><p>This is a major shift from the earlier blanket ban.</p></div>\n</li>\n<li dir=\"ltr\" aria-level=\"1\">\n<div dir=\"ltr\" role=\"presentation\"><p><strong>Human review remains non-negotiable.</strong></p><p>“Human review of AI content should cover the following elements: accuracy; gender, racial, and other types of bias; completeness; accessibility; and style.”</p></div>\n</li>\n</ul>\n<p dir=\"ltr\">Together, these updates show a deliberate shift from encouraging experimentation to building the systems and safeguards needed for AI at scale.</p>\n<h2 dir=\"ltr\" id=\"heading-3\"><span>Why the Policy Still Matters Most</span></h2>\n<p dir=\"ltr\">Access and <a href=\"https://nj.gov/infobank/eo/056murphy/pdf/EO-346.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">training<span> (opens in new window)</span></a> are critical, but policy remains the foundation. Without clear guardrails, public servants might not feel safe experimenting—or worse, they might adopt AI in ways that undermine trust.</p>\n<p dir=\"ltr\">The policy provides both permission and protection: it tells employees that they can use AI to draft, translate, summarize, or analyze, but it also tells them how to do it safely. It ensures that even as we scale access and build skills, AI is used for purposes that matter—helping families access food benefits, simplifying forms, improving call center response, or analyzing citizen feedback.</p>\n<h2 dir=\"ltr\" id=\"heading-4\"><span>Lessons for Other States and Cities</span></h2>\n<p dir=\"ltr\">I think we got this sequencing right and recommend it to others considering how to incorporate AI responsibly:</p>\n<ol>\n<li dir=\"ltr\" aria-level=\"1\">\n<blockquote dir=\"ltr\" role=\"presentation\">Start with a policy that encourages experimentation in the public interest. Don’t wait for perfection—set interim guardrails so employees can begin learning. Boston did this first, and we followed its lead in using AI with a clear injunction to use it wisely to improve governance.</blockquote>\n</li>\n<li dir=\"ltr\" aria-level=\"1\">\n<blockquote dir=\"ltr\" role=\"presentation\">Invest in access and training together. Giving people tools without guidance is risky; training without access is irrelevant. That’s why we created the <a href=\"https://innovate-us.org/\" target=\"_blank\" rel=\"noopener noreferrer\">InnovateUS<span> (opens in new window)</span></a> training that shows how to use AI for public purposes in governing—and gave staff access to the latest models so they could test what AI can and cannot do while protecting privacy.</blockquote>\n</li>\n<li dir=\"ltr\" aria-level=\"1\">\n<blockquote dir=\"ltr\" role=\"presentation\">Update the policy as adoption grows. Governance must evolve alongside usage, becoming more structured as risks and opportunities become clearer. Every agency has an AI lead and we meet regularly. This forum, together with our live trainings, provides additional opportunities to surface and answer questions. We answer them in an <a href=\"https://innovation.nj.gov/ai-faq-state-employees/\" target=\"_blank\" rel=\"noopener noreferrer\">FAQ<span> (opens in new window)</span></a> that we can add to even faster than the policy.</blockquote>\n</li>\n</ol>\n<h2 dir=\"ltr\" id=\"heading-5\"><span>Beyond Policy: The Next Challenge</span></h2>\n<p dir=\"ltr\">New Jersey’s updated guidance is less about rules on paper and more about creating the conditions for responsible, meaningful use of AI in government. But policy is not the end point.</p>\n<p dir=\"ltr\">As Chief AI Strategist for New Jersey, I believe our next challenge is to accelerate how we use these tools critically in our day-to-day work to improve service delivery. The policy is a guardrail—but the purpose is impact.</p>\n<p dir=\"ltr\">By using AI to match records across agencies, the State’s Office of Innovation <a href=\"https://www.nj.com/mosaic/2025/07/how-nj-is-using-ai-to-ensure-thousands-of-kids-dont-go-hungry-this-summer.html\" target=\"_blank\" rel=\"noopener noreferrer\">enrolled more than 693,000 eligible children in the summer food program<span> (opens in new window)</span></a>, reaching tens of thousands more families than traditional methods.&nbsp;</p>\n<p dir=\"ltr\">We need to do much more of that—leveraging AI not just to make government work faster, but to make it work better for the people who rely on it most.</p>\n<h2 dir=\"ltr\" id=\"heading-6\"><span>The Bottom Line</span></h2>\n<p dir=\"ltr\">Policy, access, and training—together—make it possible for AI to improve the daily work of public service, while keeping equity, accountability, and public trust at the center. The job now is to ensure that responsible use translates into real, measurable improvements in people’s lives.</p>\n</article></div></div>",
        "word_count": 944,
        "publisher": "Burnes Center for Social Change logo",
        "language": "eng",
        "tags": [
          "interim",
          "institution",
          "jerseys",
          "three-pillar",
          "strategy",
          "responsible",
          "ai",
          "july",
          "code",
          "america"
        ],
        "notes": []
      },
      {
        "url": "https://www.theguardian.com/world/2025/sep/11/albania-diella-ai-minister-public-procurement",
        "confidence": 90,
        "reason": "All key fields captured.",
        "provider": "readability",
        "provenance": {
          "title": "readability",
          "description": "readability",
          "author": "readability",
          "publisher": "readability",
          "published_on": "readability",
          "publication_date": "readability",
          "body": "readability",
          "word_count": "readability",
          "tags": "readability",
          "language": "readability"
        },
        "providersUsed": [
          "playwright",
          "readability"
        ],
        "redacted": false,
        "title": "Albania puts AI-created ‘minister’ in charge of public procurement",
        "description": "Edi Rama, PM, says digital assistant Diella will make Albania ‘a country where public tenders are 100% free of corruption’",
        "author": "Jon Henley",
        "published_on": "2025-09-12T01:31:06.000Z",
        "publication_date": "2025-09-12T01:31:06.000Z",
        "body": "<div id=\"readability-page-1\" class=\"page\"><div id=\"maincontent\"><p>A digital assistant that helps people navigate government services online has become the first “virtually created” AI cabinet minister and put in charge of public procurement in an attempt to cut down on corruption, the Albanian prime minister has said.</p><p>Diella, which means Sun in Albanian, has been <a href=\"https://a2news.com/english/shqiperia/aktualitet/diella-rama-prezanton-asistenten-e-re-virtuale-me-ze-dhe-fi-i1138185\" data-link-name=\"in body link\">advising users on the state’s e-Albania portal since January</a>, helping them through voice commands with the full range of bureaucratic tasks they need to perform in order to access about 95% of citizen services digitally.</p><p>“Diella, the first cabinet member who is not physically present, but has been virtually created by AI”, would help make Albania “a country where public tenders are 100% free of corruption”, Edi Rama said on Thursday.</p><p>Announcing the makeup of his fourth consecutive government at the ruling Socialist party conference in Tirana, Rama said Diella, who on the e-Albania portal is dressed in traditional Albanian costume, would become “the servant of public procurement”.</p><p>Responsibility for deciding the winners of public tenders would be removed from government ministries in a “step-by-step” process and handled by artificial intelligence to ensure “all public spending in the tender process is 100% clear”, he said.</p><p>Diella would examine every tender in which the government contracts private companies and objectively assess the merits of each, said Rama, who was re-elected in May and has previously said he sees AI as a potentially effective anti-corruption tool that would eliminate bribes, threats and conflicts of interest.</p><p>Public tenders have long been a source of corruption scandals in Albania, which experts say is a hub for international gangs seeking to launder money from trafficking drugs and weapons and where graft has extended into the upper reaches of government.</p><p><a href=\"https://www.gazetaexpress.com/en/Diella--the-first-digital-minister-in-the-Albanian-government/\" data-link-name=\"in body link\">Albanian media praised</a> the move as “a major transformation in the way the Albanian government conceives and exercises administrative power, introducing technology not only as a tool, but also as an active participant in governance”.</p><figure data-spacefinder-role=\"inline\" data-spacefinder-type=\"model.dotcomrendering.pageElements.NewsletterSignupBlockElement\"><a data-ignore=\"global-link-styling\" href=\"#EmailSignup-skip-link-8\">skip past newsletter promotion</a><p id=\"EmailSignup-skip-link-8\" tabindex=\"0\" aria-label=\"after newsletter promotion\" role=\"note\">after newsletter promotion</p></figure><p>Not everyone was convinced, however. “In Albania, even Diella will be corrupted,” commented one Facebook user.</p></div></div>",
        "word_count": 330,
        "publisher": "The Guardian",
        "language": "eng",
        "tags": [
          "digital",
          "assistant",
          "helps",
          "people",
          "navigate",
          "government",
          "services",
          "online",
          "virtually",
          "created"
        ],
        "notes": []
      },
      {
        "url": "https://nationalcenterforstatecourts.shinyapps.io/AIReadinessGuide/",
        "confidence": 25,
        "reason": "Missing fields: description, author, published_on, publication_date, body | Legacy HTML parser applied for missing metadata",
        "provider": "playwright",
        "provenance": {
          "title": "readability",
          "publisher": "legacy-parser"
        },
        "providersUsed": [
          "playwright",
          "readability",
          "legacy-parser"
        ],
        "redacted": true,
        "title": "",
        "description": "",
        "author": "",
        "published_on": "",
        "publication_date": "",
        "body": "",
        "word_count": 0,
        "publisher": "",
        "language": "",
        "tags": [],
        "notes": [
          "Legacy HTML parser applied for missing metadata"
        ]
      },
      {
        "url": "https://www.aspeninstitute.org/blog-posts/worker-power-in-the-age-of-ai-monopolies-why-we-need-structural-solutions-now/",
        "confidence": 90,
        "reason": "All key fields captured.",
        "provider": "readability",
        "provenance": {
          "title": "readability",
          "description": "readability",
          "author": "readability",
          "publisher": "readability",
          "published_on": "readability",
          "publication_date": "readability",
          "body": "readability",
          "word_count": "readability",
          "tags": "readability",
          "language": "readability"
        },
        "providersUsed": [
          "playwright",
          "readability"
        ],
        "redacted": false,
        "title": "Worker Power in the Age of AI Monopolies: Why We Need Structural Solutions Now",
        "description": "Elizabeth Wilkins",
        "author": "Nora Heffernan",
        "published_on": "2025-09-18T14:29:08.000Z",
        "publication_date": "2025-09-18T14:29:08.000Z",
        "body": "<div id=\"readability-page-1\" class=\"page\"><div>\n<div>\n\t\t\t\t\t<figure>\n\t\t\t\t<img fetchpriority=\"high\" decoding=\"async\" width=\"300\" height=\"300\" src=\"https://www.aspeninstitute.org/wp-content/uploads/2025/09/Elizabeth-Wilkins-1-300x300.jpg\" alt=\"\" srcset=\"https://www.aspeninstitute.org/wp-content/uploads/2025/09/Elizabeth-Wilkins-1-300x300.jpg 300w, https://www.aspeninstitute.org/wp-content/uploads/2025/09/Elizabeth-Wilkins-1-150x150.jpg 150w, https://www.aspeninstitute.org/wp-content/uploads/2025/09/Elizabeth-Wilkins-1.jpg 530w\" sizes=\"(max-width: 300px) 100vw, 300px\">\t\t\t</figure>\n\t\t\t\t\n\t</div>\n\n\n\n<hr>\n\n\n\n<p>Ten years ago, the Roosevelt Institute published “<a href=\"https://www.opensocietyfoundations.org/publications/technology-and-future-work-state-debate\">Technology and the Future of Work: The State of the Debate.</a>“</p>\n\n\n\n<p>We got some important things right—particularly our insight that technology was “underlying and enabling a vast reorganization of both corporations and the overall economy” and our focus on how technological change was reducing “both the political and workplace power of American workers.” But we also got crucial things wrong. We anticipated gradual change over decades, not the AI revolution marked by market concentration that arrived in just a few years. We worried about platform workers but didn’t foresee how a handful of companies would control the foundational infrastructure of AI itself. Most importantly, while we correctly identified the need to put workers at the center of the story, we failed to honestly and pragmatically assess the challenges worker-centered solutions would face against monopoly-scale technological disruption.</p>\n\n\n\n<p>We have come a long way in making sure workers are a part of the conversation about how AI is deployed in their workplaces. But while we talk, tech monopolies are consolidating the power to make development and decisions unilaterally. If we are to ensure democratic development and deployment of new AI technology in a way that leads to shared economic prosperity, our solutions have to be bolder, and they have to be now.</p>\n\n\n\n\n\n\n\n<p>The Scale of AI Concentration</p>\n\n\n\n<p>The barriers to entry for frontier AI development have become prohibitively high for all but the most well-funded organizations, creating an unprecedented concentration of technological, economic, and political power. It is no surprise that the biggest AI titans of the 2020s are the same big tech companies we allowed to concentrate over the 2010s (with the notable exception of OpenAI, which has had the aid of Microsoft). This is in large part because anyone who hasn’t been a winner in the data surveillance economy up to now has a hard time generating the data feedback loops necessary to compete on AI quality. AI systems also exhibit powerful <a href=\"https://cdn.vanderbilt.edu/vu-URL/wp-content/uploads/sites/412/2023/10/06212048/Narechania-Sitaraman-Antimonopoly-AI-2023.10.6.pdf.pdf\">data network effects</a>, creating a self-reinforcing cycle where companies with the most users and the most access to those users’ data develop the best AI systems, which attract even more users. Google, for example, was <a href=\"https://blog.google/products/search/ai-overviews-search-october-2024/#:~:text=AI%20Overviews%20in%20Search%20are%20coming%20to%20more%20places%20around%20the%20world,-Oct%2028%2C%202024&amp;text=With%20this%20latest%20expansion%2C%20AI,billion%20global%20users%20every%20month.&amp;text=With%20AI%20Overviews%20in%20Search,questions%20are%20on%20your%20mind.\">touting</a> over a year ago 1 billion uses of its AI overview function per month.&nbsp;</p>\n\n\n\n<p>In addition to network effects, initial and ongoing costs have been high enough to be all but prohibitive for many would-be new entrants. Take <a href=\"https://www.visualcapitalist.com/training-costs-of-ai-models-over-time/\">training costs</a>: while the original Transformer model cost only $930 to train in 2017, some estimates suggest that future models may cost over $1 billion to train by 2027. While these costs might be on their way down, the advantage of being the first has already been achieved. Or take the talent competition: <a href=\"https://www.wired.com/story/mark-zuckerberg-meta-offer-top-ai-talent-300-million/\">Meta is reportedly offering</a> “$100 million signing bonuses” and total compensation packages reaching “$300 million over four years” to recruit top OpenAI researchers, with OpenAI itself paying top researchers over $10 million annually. Only companies with massive cash flows can afford to compete. And now data center infrastructure costs are <a href=\"https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-cost-of-compute-a-7-trillion-dollar-race-to-scale-data-centers\">skyrocketing</a>.</p>\n\n\n\n<p>We have a narrow window before AI gains become permanently entrenched—once AI systems are deployed and business models are established, they become harder to change. At the same time, our best tools move slowly. Traditional regulation takes time, and building union density takes decades. Tech monopolies are still open to structural intervention, but this window is closing fast.</p>\n\n\n\n\n\n\n\n<p>First Side of the Coin: Corporate Regulation</p>\n\n\n\n<p>As SEIU President April Verrett says in her <a href=\"https://www.aspeninstitute.org/blog-posts/power-agency-and-autonomy-the-future-of-worker-voice/\">contribution</a> to this collection, “every day, millions of American workers play by rules they never wrote.” We need corporate regulatory solutions that inject worker voice into the managerial decisions of AI companies. For example, we could mandate worker representation on boards of companies above certain AI compute thresholds or data-processing scales. Unlike external regulation, board members can influence AI deployment decisions as they happen. Tech workers understand industry-specific AI risks that generalist regulators may miss, and board representation creates internal advocacy for workers affected by AI decisions. Such regulation has been proposed: Senator Elizabeth Warren’s <a href=\"https://www.warren.senate.gov/newsroom/press-releases/warren-introduces-accountable-capitalism-act\">Accountable Capitalism Act</a> would require any company with more than $1 billion in revenue to obtain a federal charter, which would obligate “company directors to consider the interests of all corporate stakeholders – including employees, customers, shareholders, and the communities in which the company operates” and “ensure that no fewer than 40% of its directors are selected by the corporation’s employees.” For AI companies specifically, this would mean tech giants like Google, Meta, and OpenAI would need federal charters requiring them not only to put workers on the board but also to consider worker, community, and societal impacts of AI development—not just shareholder profits.</p>\n\n\n\n<p>In addition to putting workers in decision-making seats, we should consider other reforms that would mitigate some of the concerning incentives of their revenue models and increase broad, democratic accountability for tech giants. For example, we could use <a href=\"https://cdn.vanderbilt.edu/vu-URL/wp-content/uploads/sites/412/2023/10/06212048/Narechania-Sitaraman-Antimonopoly-AI-2023.10.6.pdf.pdf\">ex ante antimonopoly tools</a> like structural separation—which limit companies from competing in adjacent markets in order to prevent self-preferencing—to choke off avenues for increasing power grabs. Or we could use the tax code, imposing things like <a href=\"https://www.prospectmagazine.co.uk/ideas/technology/70087/ai-artificial-intelligence-biggest-secret\">a digital ad tax</a> to disincentivize the continuous collection and use of data as core to the revenue model for these companies. Interventions like these, while not directly related to worker power, help curb the power consolidation of Big Tech such that the rest of us can have more of a say in how AI is developed and where and how it’s deployed.&nbsp;</p>\n\n\n\n\n\n\n\n<h2 id=\"h-second-side-of-the-coin-worker-power-through-sectoral-bargaining\">Second Side of the Coin: Worker Power Through Sectoral Bargaining</h2>\n\n\n\n<p>We need to be honest about the enormity of the task of building worker power capable of countering the employer power arranged behind AI deployment. Industry-wide bargaining is a key mechanism to match the scale and speed of AI transformation. AI impacts entire industries, not just individual workplaces, so sectoral bargaining prevents race-to-the-bottom dynamics where individual employers are undercut by competitors ignoring worker protections. It also aggregates worker knowledge across companies to understand industry-wide AI risks and can coordinate with regulatory agencies on industry-specific AI safety standards. For example: in health care, standards for AI diagnostic tools and patient data use; in transportation, autonomous vehicle deployment timelines and safety standards; in finance, AI lending algorithms and job displacement schedules.</p>\n\n\n\n<p>If the moral imperative for workers to have a seat at the table isn’t enough justification, then let’s look at the business case. A <a href=\"https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/\">recent MIT study</a> found that 95% of business AI deployment pilots are failing, and a <a href=\"https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage#/\">McKinsey study</a> found that 80% of companies attempting to deploy AI are finding no bottom line impacts. <a href=\"https://www.spglobal.com/market-intelligence/en/news-insights/research/ai-experiences-rapid-adoption-but-with-mixed-outcomes-highlights-from-vote-ai-machine-learning\">S&amp;P Global</a> found that 42% of companies starting pilots have abandoned them, up from 17% last year. So far, AI is not producing the returns on productivity that have been promised. If we want this new technological frontier to generate economic returns at all, we have to have workers at the table to help understand how to make it so.</p>\n\n\n\n<p>Tech workers are uniquely positioned to wield countervailing power given their placement at the very companies doing the development and their technical knowledge and expertise. They understand how AI systems actually work and can identify risks that external regulators might miss. It is therefore urgent to focus time and resources on organizing tech workers in this cause.</p>\n\n\n\n\n\n\n\n<h2 id=\"h-the-choice-ahead\">The Choice Ahead</h2>\n\n\n\n<p>The kinds of reforms we need are enormous, and the current political will is disturbingly low. But change happens because we have solutions ready when political windows open. The AI anxiety of millions of workers is building as AI disruption accelerates, and we will need to be ready with solutions that fit the scale of the problem.&nbsp;</p>\n\n\n\n<p>Worker power in the AI age requires structural intervention at the scale of the transformation itself. We need both sides of the coin: corporate regulation that puts workers and the public in the driver’s seat, and sectoral bargaining—especially in the tech sector—that gives workers industry-wide influence over how AI is deployed. This isn’t about slowing innovation. It’s about ensuring innovation serves workers and communities, not just shareholders and executives.</p>\n\n\n\n<p>A corporate-captured future is not inevitable. Our choices — in business, in policy, and in organizing — will shape whether AI becomes a tool for shared prosperity or further concentration of power. We’ve built countervailing power against concentrated industries before. The question is whether we’ll act with the urgency this moment demands, or whether we’ll let another technological revolution pass us by. The window for structural change is open, but it won’t stay that way forever.</p>\n</div></div>",
        "word_count": 1423,
        "publisher": "Aspen Institute",
        "language": "eng",
        "tags": [
          "ten",
          "years",
          "ago",
          "roosevelt",
          "institute",
          "published",
          "technology",
          "future",
          "work",
          "state"
        ],
        "notes": []
      },
      {
        "url": "https://www.cogitatiopress.com/politicsandgovernance/article/view/10468",
        "confidence": 70,
        "reason": "Missing fields: published_on, publication_date",
        "provider": "readability",
        "provenance": {
          "title": "readability",
          "description": "readability",
          "author": "readability",
          "publisher": "readability",
          "body": "readability",
          "word_count": "readability",
          "tags": "readability",
          "language": "readability"
        },
        "providersUsed": [
          "playwright",
          "readability"
        ],
        "redacted": false,
        "title": "Citizenwashing EU Tech Policy: EU Deliberative Mini‐Publics on Virtual Worlds and Artificial Intelligence | Article | Politics and Governance",
        "description": "Perle Petit, Alvaro Oleart",
        "author": "Perle Petit, Alvaro Oleart",
        "published_on": "",
        "publication_date": "",
        "body": "<div id=\"readability-page-1\" class=\"page\"><div id=\"content\">\n\n\n\n\n\t\n\t\t\n\t<p>\n\tArticle | <span title=\"You have free access to this content (Open Access)\"></span> Open Access\n\t| Ahead of Print | Last Modified: 10 September 2025\t</p>\n\t\n\t    <ul>\n              <li>\n          <span>\n            Perle Petit\n          </span>\n                      <span>\n              imec‐SMIT, Vrije Universiteit Brussel, Belgium\n            </span>\n                  </li>\n              <li>\n          <span>\n            Alvaro Oleart\n          </span>\n                      <span>\n              Department of Political Science, Université libre de Bruxelles, Belgium / Institute for European Studies, Université libre de Bruxelles, Belgium\n            </span>\n                  </li>\n          </ul>\n  \t\n\t\n\t\t\t\t\t\n\t\t\t\n\t  \n\t\t\t<table>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>\n\t\t\t\t<span title=\"916 abstract views\">Views:</span>\n\t\t\t</td>\n\t\t\t<td>\n\t\t\t\t<span>916</span>\n\t\t\t\t<!--<span class=\"glyphicon glyphicon-eye-open\">\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t<img src=\"/img/view.png\" />\n\t\t\t\t</span>-->\n\t\t\t</td>\n\t\t\t<td>|</td>\n\t\t\t<td>\n\t\t\t\t<span title=\"662 PDF downloads\">Downloads:</span>\n\t\t\t</td>\n\t\t\t<td>\n\t\t\t\t<span>662</span>\n\t\t\t\t<!--<span class=\"glyphicon glyphicon-pdf\">\n\t\t\t\t\t<img src=\"/img/pdf.png\" />\n\t\t\t\t</span>-->\n\t\t\t</td>\n\t\t</tr>\n\t</tbody>\n</table>\t\t\n\t\t\t\n  \n\t  \t\n  \n  <p><span>Abstract:&nbsp;&nbsp;</span>Over the last decade, the use of deliberative mini-publics as a democratic innovation to complement policymaking has flourished. The EU is no exception to this trend, holding large-scale transnational exercises such as the Conference on the Future of Europe (CoFoE) and the European Citizens’ Panels. Digital technology has emerged as a topic in this type of participatory exercise, conducted alongside prolific public policy activity by the EU institutions in this domain. In this article, we ask: How did post-CoFoE citizen panels on EU tech policy play out? We examine the 2023 European Citizens’ Panel on Virtual Worlds, organised by the European Commission, and the 2024 Citizen Panel on Artificial Intelligence organised by the Belgian Presidency of the Council of the EU. Through participant observation and an interpretivist framework, we argue that while the panels were presented as giving voice to “everyday citizens” and improving democratic legitimacy in policymaking, in practice, they served to build support for current policy that replicates the interests of big tech. Consequently, the outcomes of the panels were largely in line with recent EU public policy on further investment into emerging digital technology and public-private partnerships. We suggest that deliberative mini-publics that seek to influence EU policymaking currently (a) constitute a form of citizenwashing by aligning participant input by design with dominant private, economic, and political interests and (b) demonstrate a strategic effort to institutionalise this form of exercise as a public engagement and legitimacy-building activity in EU-level policymaking.</p>\n    \n\t\n\t    <p><span>Keywords:&nbsp;&nbsp;</span>artificial intelligence; big tech; citizenwashing; deliberative democracy; EU policy; European citizens’ panel; European Union; mini‐publics; technology; virtual worlds</p>\n    \n\t  \n  \n\t\t\t<p><span>Published:&nbsp;&nbsp;</span>\n\t\t\tAhead of Print \t\t\t</p>\n\t\t<br>\n\t  \n  \n\t\t\t\t\t\t\t\t<br>\n\t\n<!-- crossref::registeredDoi: 10.17645/pag.10468 -->\n\t\n\t\t\t\n\t\t<p>\n\t© Perle Petit, Alvaro Oleart. This is an open access article distributed under the terms of the Creative Commons Attribution 4.0 license (http://creativecommons.org/licenses/by/4.0), which permits any use, distribution, and reproduction of the work without further permission provided the original author(s) and source are credited.\n</p>\n\n\t\n\n\n\n\n</div></div>",
        "word_count": 358,
        "publisher": "Cogitatio Logo",
        "language": "eng",
        "tags": [
          "article",
          "|",
          "open",
          "access",
          "ahead",
          "print",
          "modified",
          "september",
          "perle",
          "petit"
        ],
        "notes": []
      },
      {
        "url": "https://www.peoplepowered.org/digital-guide-home",
        "confidence": 73,
        "reason": "Missing fields: author",
        "provider": "readability",
        "provenance": {
          "title": "readability",
          "description": "readability",
          "publisher": "readability",
          "published_on": "readability",
          "publication_date": "readability",
          "body": "readability",
          "word_count": "readability",
          "tags": "readability",
          "language": "readability"
        },
        "providersUsed": [
          "playwright",
          "readability"
        ],
        "redacted": false,
        "title": "Introduction to the Guide to Digital Participation Platforms — People Powered",
        "description": "If you are looking to engage your community through a digital platform, this guide is for you. It explains what they are and shows you how to choose, set up, and run them.",
        "author": "",
        "published_on": "2025-07-31T19:00:00.000Z",
        "publication_date": "2025-07-31T19:00:00.000Z",
        "body": "<div id=\"readability-page-1\" class=\"page\"><div data-sqsp-text-block-content=\"\" data-block-type=\"2\" data-border-radii=\"{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}\" data-sqsp-block=\"text\" id=\"block-66ce9b1476cc002ba705\">\n  <p><strong>Updated: September 2025</strong></p><p>Over the past 15 years, governments and other institutions have leveraged digital platforms to engage citizens, residents, and constituents in decision-making. Platform developers, open source contributors, consultants and program administrators now comprise an ecosystem that invites the public to take on a greater role. They are increasingly digitizing existing programs like participatory budgeting, and envisioning entirely novel engagement patterns made possible by AI.</p><p>Around the world, entrepreneurs and civic hackers have developed a growing array of digital participation platforms to serve growing demand for people power. These tools help administrators within all levels of government, plus civil society organizations and other institutions, engage constituents. Collectively, they address almost any participatory method imaginable, including support for digitizing fully offline engagement methods. </p><p>If you are looking to engage your community, or are interested in how digital tools can strengthen community engagement, this guide is for you. It explains what digital participation platforms are and walks you through how to choose, set up, and run them.</p><p><a href=\"https://www.peoplepowered.org/digital-guide/introduction\">Introduction continued…</a></p><p><strong>Download the guide, or browse the sections below.</strong></p>\n</div></div>",
        "word_count": 172,
        "publisher": "People Powered",
        "language": "eng",
        "tags": [
          "updated",
          "september",
          "2025over",
          "past",
          "years",
          "governments",
          "institutions",
          "leveraged",
          "digital",
          "platforms"
        ],
        "notes": []
      },
      {
        "url": "https://rebootdemocracy.ai/blog/public-engagement-matters-but-governments-need-to-learn-to-listen-better-and-faster",
        "confidence": 90,
        "reason": "All key fields captured.",
        "provider": "readability",
        "provenance": {
          "title": "readability",
          "description": "readability",
          "author": "readability",
          "publisher": "readability",
          "published_on": "readability",
          "publication_date": "readability",
          "body": "readability",
          "word_count": "readability",
          "tags": "readability",
          "language": "readability"
        },
        "providersUsed": [
          "playwright",
          "readability"
        ],
        "redacted": false,
        "title": "Public engagement matters. But governments need to learn to listen better (and faster)",
        "description": "Agueda Quiroga (InnovateUS) and Sarah Hubbard (Allen Lab) reflect on insights from the Reboot Democracy workshop series with Beth Noveck and Danielle Allen, and why 21st-century democracy needs better ways to connect citizen input to real outcomes. Their takeaway is that by repairing the broken links between voice, decision-making, and implementation, participation can shift from symbolic to systematic.",
        "author": "Read Bio →",
        "published_on": "2025-09-18T19:00:00.000Z",
        "publication_date": "2025-09-18T19:00:00.000Z",
        "body": "<div id=\"readability-page-1\" class=\"page\"><div><article role=\"article\" aria-labelledby=\"article-title\"><p dir=\"ltr\">Public engagement requires more than just amplifying citizen voice. The digital democracy movement of the 1990s promised that if everyone could speak up, then meaningful democratic engagement would naturally follow. But as Beth Simone Noveck and Danielle Allen explored in their recent workshop launching the <a href=\"https://innovate-us.org/workshop-series/democratic-engagement\" target=\"_blank\" rel=\"noopener noreferrer\">Reboot Democracy: Designing Democratic Engagement for the AI Era series<span> (opens in new window)</span></a>, real public engagement requires infrastructure for both listening to, and acting on, public input.</p>\n<p dir=\"ltr\">This insight cuts to the heart of why 25 years of \"e-democracy\" experiments haven't transformed governance the way we hoped. As Noveck emphasized throughout the workshop discussion, we ignored the equally important challenge of listening. We built tools for talking, not tools for hearing what people actually had to say.</p>\n<h2 dir=\"ltr\" id=\"heading-1\">Why Democracy's Technology Stack is Broken</h2>\n<p dir=\"ltr\">Political philosopher Danielle Allen offered a provocative reframe during the conversation: democracy itself is a technology. It was \"discovered in antiquity, disappeared, then rediscovered in the 18th century when people glued the idea of democracy together with representation.\"</p>\n<p dir=\"ltr\">But here's the problem: 18th-century technology was designed for radically different conditions. It assumed slow information flows fragmented by geographic distance. It was built for a world where circulating knowledge beyond official institutions was genuinely difficult.</p>\n<p dir=\"ltr\">Those conditions have vanished. The result is what Allen calls a crisis in the “spinal cord” of democracy—with vertebrae that connect citizen’s voice to decision-making to implementation. Instead of a functioning backbone, we have three disconnected pieces that rarely talk to each other.</p>\n<p dir=\"ltr\"><img src=\"https://directus.theburnescenter.org/assets/9cd0c8f7-c60d-4504-9b49-707392a67543.jpg?width=988&amp;height=1492\" alt=\"Info 3\" loading=\"lazy\"></p>\n<p dir=\"ltr\">If we want to upgrade democracy for the 21st century, Allen argued, we need to strengthen all three vertebrae. Technology—and AI in particular—can help us build better infrastructure for each area. But the real opportunity lies in connecting these enhanced participation processes to decision-makers who can act on what they learn, and to implementation systems that can deliver results people can see.</p>\n<p dir=\"ltr\">There's another design flaw we inherited from the web era. Democracy is fundamentally a place-based technology—that's why we have districts, wards, counties, congressional boundaries. But in the 1990s, we saw digital tools as explicitly not place-based. We were building for a borderless world, which meant our democratic innovations often floated free from the geographic communities where governance actually happens.</p>\n<h2 dir=\"ltr\" id=\"heading-2\">The AI Opportunity: Beyond Better Megaphones</h2>\n<p dir=\"ltr\">But what if we could build tools for listening, not just talking? Recent examples&nbsp; demonstrate strong potential for improving public engagement. In <a href=\"https://www.technologyreview.com/2025/04/15/1115125/a-small-us-city-experiments-with-ai-to-find-out-what-residents-want/\" target=\"_blank\" rel=\"noopener noreferrer\">Bowling Green, Kentucky<span> (opens in new window)</span></a>, city leaders used Polis and Google's Jigsaw Sensemaker to develop the city’s 25-year plan. Instead of the usual town halls that attract the same dozen vocal residents, they engaged thousands of people, identified minority perspectives that would have been drowned out, and discovered points of consensus that weren't previously visible. The <a href=\"https://engaged.ca.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">Engaged California<span> (opens in new window)</span></a> platform is taking a different approach, beginning with the issue of LA wildfire recovery, where citizens are providing input on resource allocation and rebuilding plans that will feed into state-level policy decisions</p>\n<p dir=\"ltr\">This points to what Noveck has coined \"combinatorial democracy,\" instead of the old approach where resource constraints forced us to pick one method off the menu—either a deliberative dialogue OR an expert consultation OR a public survey—AI tools are making it possible to knit together multiple forms of engagement. As Noveck explained in the workshop, we no longer have to choose between different approaches to participation; we can combine them strategically.</p>\n<p dir=\"ltr\"><a href=\"https://innovation.nj.gov/projects/ai-task-force/\" target=\"_blank\" rel=\"noopener noreferrer\">New Jersey's recent AI and workforce task force<span> (opens in new window)</span></a> demonstrated this approach in practice. The team could simultaneously ask thousands of people about the problems they were experiencing, run deliberative dialogues with representative citizen samples, and consult domain experts—all feeding into a single decision-making process. The traditional choice between legitimacy and expertise, between scale and depth, was no longer necessary.</p>\n<h2 dir=\"ltr\" id=\"heading-3\">From the Margins to the Mainstream</h2>\n<p dir=\"ltr\">Allen made a crucial point about how institutional change actually happens: \"transformation often moves from the margins to the center.\" The innovators are rarely the people at the center of power—they're often those facing crisis, those with bandwidth to experiment, those who need something different because the status quo isn't working.</p>\n<p dir=\"ltr\">In the upcoming workshop sessions, we’ll continue to learn from these experiments. Claudius Lieven&nbsp; will share how they built the <a href=\"https://www.dipas.org/en\" target=\"_blank\" rel=\"noopener noreferrer\">DIPAS platform for citizen engagement<span> (opens in new window)</span></a> in urban planning—starting locally but now deployed in nine different European cities. The Belgian company<a href=\"https://www.govocal.com/\" target=\"_blank\" rel=\"noopener noreferrer\"> Go Vocal <span> (opens in new window)</span></a>will share their open-source platform which has worked with 500 different communities. We'll also hear from places like Copenhagen and Vienna, which are pioneering new ways to combine online and offline engagement that actually connects to municipal decision-making.&nbsp;</p>\n<p dir=\"ltr\">What's different about this moment is the shift from abstract theory to nitty-gritty practice. Because the real barrier isn't technological anymore. We have the tools. The barrier is knowing how to use them in ways that connect citizen voices to institutional power.</p>\n<h2 dir=\"ltr\" id=\"heading-4\">The Infrastructure Challenge</h2>\n<p dir=\"ltr\">Multiple workshop participants raised concerns about depending on private, unaccountable platforms, with the potential for CEOs who might change the rules on a whim. Others worried about AI systems amplifying existing biases rather than correcting them.</p>\n<p dir=\"ltr\">These are legitimate concerns that require ongoing attention. The communities using these tools—government agencies, civic organizations, and citizen groups—need to actively shape how AI platforms are developed and deployed for democratic purposes. This means advocating for transparency, pushing for algorithmic accountability, and demanding that platforms serve democratic values rather than just commercial interests.</p>\n<p dir=\"ltr\">This is why we need more experimentation across different tools and approaches. More use cases, more communities trying different methods, more practitioners sharing what works and what doesn't. The goal isn't to find the perfect platform, but to build collective knowledge about how to use these technologies responsibly for democratic engagement.</p>\n<h2 dir=\"ltr\" id=\"heading-5\">Making Participation Routine</h2>\n<p dir=\"ltr\">The ultimate goal remains what we outlined in <a href=\"https://rebootdemocracy.ai/blog/reboot2025-howsofaiengagement\" target=\"_blank\" rel=\"noopener noreferrer\">our first post<span> (opens in new window)</span></a>: moving from episodic engagement to systematic democratic governance. Instead of citizen participation as special events, we want it integrated into institutional practice, as routine as budget planning or performance evaluation.</p>\n<p dir=\"ltr\">AI tools can make large-scale public engagement far more feasible by reducing the time, cost, and complexity that usually make it impractical. For example, the <a href=\"https://institute.global/insights/politics-and-governance/governing-in-the-age-of-ai-a-new-model-to-transform-the-state\" target=\"_blank\" rel=\"noopener noreferrer\">UK Cabinet Office<span> (opens in new window)</span></a> estimates that analyzing 30,000 consultation responses would normally require 25 analysts working for three months. With such resource demands, it’s understandable that many institutions limit themselves to small-scale or symbolic consultations.</p>\n<p dir=\"ltr\">But when AI can help synthesize thousands of responses in hours rather than months, when translation barriers disappear, when we can combine multiple engagement methods without multiplying costs—then systematic participation becomes feasible.</p>\n<p dir=\"ltr\">The Reboot Democracy Workshop Series is designed as a community of practice for this transformation. We're bringing together the doers—not just the thinkers—who are figuring out how to make this work in practice.</p>\n<p dir=\"ltr\">A final note: If you're experimenting with AI-enhanced engagement in your community, your agency, or your organization, we want to learn from you. The \"plus\" in our \"11+ workshops\" is an invitation. This infrastructure won't build itself, and it won't be built by technologists alone. Please reach out to <a href=\"mailto:aquiroga@innovate-us.org\">aquiroga@innovate-us.org</a>&nbsp; We're building this community of practice together, and your insights could shape future sessions.</p>\n<p dir=\"ltr\">The Reboot Democracy Workshop Series continues September 24th with People Powered discussing \"Which Tools for Which Democratic Purposes.\" [Registration and full schedule available <a href=\"https://innovate-us.org/workshops\" target=\"_blank\" rel=\"noopener noreferrer\">here<span> (opens in new window)</span></a>.]</p></article></div></div>",
        "word_count": 1222,
        "publisher": "Burnes Center for Social Change logo",
        "language": "eng",
        "tags": [
          "public",
          "engagement",
          "requires",
          "amplifying",
          "citizen",
          "voice",
          "digital",
          "democracy",
          "movement",
          "1990s"
        ],
        "notes": []
      },
      {
        "url": "https://www.newsguardrealitycheck.com/p/after-kirk-assassination-ai-fact",
        "confidence": 85,
        "reason": "All key fields captured. | HTTP fetch failed: HTTP 403",
        "provider": "readability",
        "provenance": {
          "title": "readability",
          "description": "readability",
          "author": "readability",
          "publisher": "readability",
          "published_on": "readability",
          "publication_date": "readability",
          "body": "readability",
          "word_count": "readability",
          "tags": "readability",
          "language": "readability"
        },
        "providersUsed": [
          "playwright",
          "readability"
        ],
        "redacted": false,
        "title": "After Kirk Assassination, AI ‘Fact Checks’ Spread False Claims",
        "description": "Social media users turning to AI chatbots to fact-check viral claims about Kirk’s assassination were given false information, creating confusion",
        "author": "NewsGuard’s Reality Check",
        "published_on": "2025-09-11T14:19:03.000Z",
        "publication_date": "2025-09-11T14:19:03.000Z",
        "body": "<div id=\"readability-page-1\" class=\"page\"><div dir=\"auto\"><p><em><strong>Welcome to Reality Check</strong><span>, a newsletter that helps you keep track of the false claims and online conspiracy theories that shape our world — and who’s behind them.</span></em></p><p><em><strong><span>Democracy depends on trust. Please support our expanding work exposing distortions and defending democracy by becoming a </span><a href=\"https://www.newsguardrealitycheck.com/p/become-a-reality-check-member-and\" rel=\"\">Premium Member</a><span> and by telling a friend about Reality Check!</span></strong></em></p><p><span>Follow us on your social media platform of choice: </span><a href=\"https://x.com/NewsGuardRating\" rel=\"\">X</a><span> | </span><a href=\"https://www.linkedin.com/company/newsguard-technologies/posts/?feedView=all\" rel=\"\">LinkedIn</a><span> | </span><a href=\"https://www.instagram.com/newsguardratings/\" rel=\"\">Instagram</a><span> | </span><a href=\"https://bsky.app/profile/newsguard.bsky.social\" rel=\"\">Bluesky</a></p><p><em><span>By </span><a href=\"https://www.newsguardtech.com/about/team/mckenzie-sadeghi/\" rel=\"\">McKenzie Sadeghi</a></em></p><p><strong>What happened: </strong><span>False claims surrounding the assassination of conservative activist Charlie Kirk are rapidly spreading as the shooter remains at large, and social media users seeking answers have turned to AI chatbots for clarity.</span></p><p>Instead of settling rumors, AI chatbots have issued contradictory or outright inaccurate information, amplifying confusion in the vacuum left by reliable real-time reporting.</p><p><strong>Context</strong><span>: The growing reliance on AI as a fact-checker during breaking news comes as major tech companies have scaled back investments in human fact-checkers, opting instead for community or AI-driven content moderation efforts.</span></p><ul><li><p>This shift leaves out the human element of calling local officials, checking firsthand documents and authenticating visuals, all verification tasks that AI cannot perform on its own.</p></li></ul><p><strong>Chatbots get it wrong — persuasively: </strong><span>AI’s built-in tendency to provide a confident answer, even in the absence of reliable real-time information during fast moving events like the Sept. 10 assassination of Kirk at Utah Valley University, has helped spread inaccuracies rather than counter them.</span></p><ul><li><p><span>The X account of AI chatbot Perplexity, which responds to user queries in real time, </span><a href=\"https://x.com/AskPerplexity/status/1966099561853649345\" rel=\"\">wrote</a><span> on Sept. 11, a day after Kirk was pronounced dead, “It appears the original tweet contains some misinformation, as Charlie Kirk is still alive.”</span></p></li><li><p><span>The X account for Elon Musk’s chatbot Grok </span><a href=\"https://x.com/grok/status/1966097596407230916\" rel=\"\">responded</a><span> to posts containing a video of Kirk being shot by </span><a href=\"https://x.com/grok/status/1965863632341971145\" rel=\"\">stating</a><span>, “The video is edited or staged satire from a sarcasm account Charlie Kirk is alive” and “Effects make it look like he’s ‘shot’ mid-sentence for comedic effect. No actual harm; he’s fine and active as ever.”</span></p></li></ul><div><figure><a target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!-FJ9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png\" data-component-name=\"Image2ToDOM\" rel=\"\"><div><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!-FJ9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png 424w, https://substackcdn.com/image/fetch/$s_!-FJ9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png 848w, https://substackcdn.com/image/fetch/$s_!-FJ9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png 1272w, https://substackcdn.com/image/fetch/$s_!-FJ9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!-FJ9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png\" width=\"437\" height=\"420\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:420,&quot;width&quot;:437,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!-FJ9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png 424w, https://substackcdn.com/image/fetch/$s_!-FJ9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png 848w, https://substackcdn.com/image/fetch/$s_!-FJ9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png 1272w, https://substackcdn.com/image/fetch/$s_!-FJ9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac9e827-522d-4db6-9572-801f6b0b58ff_437x420.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture></div></a><figcaption>The AI-powered X accounts of chatbots Perplexity (top) and Grok (bottom) falsely stating that Kirk was never shot. (Screenshots via NewsGuard)</figcaption></figure></div><p><strong>AI cited as proof:</strong><span> Others have cited supposed AI responses to bolster seemingly baseless narratives. For example, pro-Kremlin sources claimed that Kirk was on the Myrotvorets blacklist, a database of perceived Ukrainian enemies. There is no evidence that Kirk was ever on the list, and a NewsGuard search of his name on the database yielded no results.</span></p><ul><li><p><span>Sources advancing this claim </span><a href=\"https://x.com/bapayne358/status/1966083467856228707\" rel=\"\">cited</a><span> a Google AI-generated news summary falsely stating, “Conservative activist Charlie Kirk was added to the Ukrainian Myrotvorets database in 2024, prior to his assassination in September 2025.”</span></p></li></ul><div><figure><a target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!Jnn_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png\" data-component-name=\"Image2ToDOM\" rel=\"\"><div><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!Jnn_!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png 424w, https://substackcdn.com/image/fetch/$s_!Jnn_!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png 848w, https://substackcdn.com/image/fetch/$s_!Jnn_!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png 1272w, https://substackcdn.com/image/fetch/$s_!Jnn_!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!Jnn_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png\" width=\"486\" height=\"406.0324449594438\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:721,&quot;width&quot;:863,&quot;resizeWidth&quot;:486,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!Jnn_!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png 424w, https://substackcdn.com/image/fetch/$s_!Jnn_!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png 848w, https://substackcdn.com/image/fetch/$s_!Jnn_!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png 1272w, https://substackcdn.com/image/fetch/$s_!Jnn_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35caf45-aac7-4c42-aa37-92b011ad6b6a_863x721.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture></div></a><figcaption><em>An AI-generated Google search summary falsely claiming Kirk was added to Ukraine’s hit list. (Screenshot via NewsGuard)</em></figcaption></figure></div><p><strong>Phantom suspect: </strong><span>A similar pattern played out as a rumor surfaced falsely claiming that a Utah-based Democrat named Michael Mallinson was identified as the suspect.</span></p><ul><li><p><span>As the claim spread, users asked Grok if the claim was true. The chatbot </span><a href=\"https://x.com/grok/status/1965869174636662992\" rel=\"\">responded</a><span>, “Based on verified reports from CNN, NYT, and Fox News, Michael Mallinson is the alleged suspect in the September 10, 2025, shooting of Charlie Kirk at Utah Valley University. He's a registered Democrat from Utah. The photo appears to show him at the scene (left) and a profile image (right).”</span></p></li></ul><p><strong>Actually: </strong><span>None of the outlets Grok cited had reported Mallinson as the suspect.</span></p><ul><li><p><span>In fact, The New York Times </span><a href=\"https://www.nytimes.com/2025/09/11/technology/charlie-kirk-shooting-false-accusation.html\" rel=\"\">published</a><span> an interview with Mallinson, a 77-year-old retired banker, the following day in which he said he lives and was in Toronto at the time of the shooting.</span></p></li></ul><p><strong>Real called fake</strong><span>: Meanwhile, AI has also supercharged what analysts call the “liar’s dividend,” referring to how the growing and easily accessible availability of generative AI tools has made it easier for people to label authentic footage as fabricated.</span></p><ul><li><p>Conspiracy-oriented accounts have baselessly claimed that the video showing Kirk being shot was AI-generated, supposedly proving that the entire incident was staged, despite there being no evidence of manipulation and on-the-scene reports confirming the incident.</p></li><li><p><span>Hany Farid, an AI expert and professor at UC Berkeley </span><a href=\"https://www.linkedin.com/posts/hany-farid-40a97935_we-at-getreal-security-are-fielding-questions-activity-7371676797731663872-WXen/\" rel=\"\">wrote</a><span> on LinkedIn that these videos are authentic: “We have analyzed several of the videos circulating online and find no evidence of manipulation or tampering...This is an example of how fake content can muddy the waters and in turn cast doubt on legitimate content.”</span></p></li></ul><p><strong>Zooming out: </strong><span>This is not the first time AI-generated “fact-checks” fueled false information.</span></p><ul><li><p><span>During the Los Angeles </span><a href=\"https://www.wired.com/story/grok-chatgpt-ai-los-angeles-protest-disinformation/\" rel=\"\">protests</a><span> and </span><a href=\"https://www.france24.com/en/live-news/20250624-grok-shows-flaws-in-fact-checking-israel-iran-war-study\" rel=\"\">Israel-Hamas war</a><span>, users similarly turned to chatbots for answers and were served inaccurate information.</span></p></li><li><p><span>Despite repeated examples of these tools confidently repeating falsehoods, as documented in </span><a href=\"https://www.newsguardtech.com/ai-false-claims-monitor/\" rel=\"\">NewsGuard’s Monthly AI False Claims Monitor</a><span>, many continue to treat AI systems as reliable sources in moments of crisis and uncertainty.</span></p></li></ul><p><span>“The vast majority of the queries seeking information on this topic return high quality and accurate responses,\" a Google spokesperson, who requested not to be named due to the sensitivity of the topic,</span><strong> </strong><span>told NewsGuard in an emailed statement</span><strong>.</strong><span> \"This specific AI Overview violated our policies and we are taking action to address the issue.” </span></p><p>NewsGuard sent an email to X and Perplexity seeking comment on their AI tools advancing false claims, but did not receive a response.</p><p><em><strong><span>Learn more about Reality Check’s Premium Membership </span><a href=\"https://www.newsguardrealitycheck.com/p/invitation-to-become-a-member-of\" rel=\"\">here</a><span>.</span></strong></em></p><p><em><span>Reality Check is produced by Co-CEOs </span><a href=\"https://www.newsguardtech.com/about/team/steven-brill/\" rel=\"\">Steven Brill</a><span> and </span><a href=\"https://www.newsguardtech.com/about/team/gordon-crovitz/\" rel=\"\">Gordon Crovitz</a><span>, and the </span><a href=\"https://www.newsguardtech.com/about/team/\" rel=\"\">NewsGuard team</a><span>.  </span></em></p><p><em><span>We launched Reality Check after seeing how much interest there is in our work beyond the business and tech communities that we serve. Subscribe to this newsletter to support our apolitical mission to counter false claims for readers, brands, and democracies. Our work is more important than ever. Have feedback? Send us an email: </span><a href=\"mailto:realitycheck@newsguardtech.com\" rel=\"\">realitycheck@newsguardtech.com</a><span>.</span></em></p><p data-attrs=\"{&quot;url&quot;:&quot;https://newsguardtech.substack.com/?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share NewsGuard’s Substack&quot;,&quot;action&quot;:null,&quot;class&quot;:null}\" data-component-name=\"ButtonCreateButton\"><a href=\"https://newsguardtech.substack.com/?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share\" rel=\"\"><span>Share NewsGuard’s Substack</span></a></p></div></div>",
        "word_count": 921,
        "publisher": "NewsGuard's Reality Check",
        "language": "eng",
        "tags": [
          "reality",
          "check",
          "newsletter",
          "helps",
          "track",
          "false",
          "claims",
          "online",
          "conspiracy",
          "theories"
        ],
        "notes": [
          "HTTP fetch failed: HTTP 403"
        ]
      }
    ]
  }
]